{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a7b096c01a6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43dfe7b6ecbcf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7174aae04944a1aa75ead5a383b278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/372 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joshu\\AppData\\Local\\r-miniconda\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\joshu\\.cache\\huggingface\\hub\\models--ZeeeWP--segformer-b0-finetuned-segments-satellite-terrain. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc702727cb84095813b31e44cc766d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5b7f3e1eef4e4b8912124be8beb0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/14.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 454 patches to combined_patches/\n"
     ]
    }
   ],
   "source": [
    "urban_processor = AutoImageProcessor.from_pretrained(\"ZeeeWP/segformer-b0-finetuned-segments-satellite-terrain\")\n",
    "urban_model = SegformerForSemanticSegmentation.from_pretrained(\"ZeeeWP/segformer-b0-finetuned-segments-satellite-terrain\")\n",
    "\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def apply_reversed_summer_colormap(ndvi_prediction):\n",
    "    # Normalize NDVI values to the range [0, 255]\n",
    "    normalized_ndvi = cv2.normalize(ndvi_prediction, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Reverse the normalized values (flip the color mapping)\n",
    "    reversed_normalized_ndvi = 255 - normalized_ndvi  # Reversing the normalized NDVI values\n",
    "    \n",
    "    # Apply the summer colormap on the reversed normalized values\n",
    "    colormap = cv2.applyColorMap(reversed_normalized_ndvi.astype(np.uint8), cv2.COLORMAP_SUMMER)\n",
    "    \n",
    "    return colormap\n",
    "\n",
    "# Function to get urbanized areas using Segformer\n",
    "def get_urban_mask(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = urban_processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = urban_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    logits = torch.nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],  # Resize to original image size\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    )\n",
    "    segmentation = torch.argmax(logits, dim=1).squeeze().cpu().numpy()  # Convert to NumPy array\n",
    "    return segmentation\n",
    "\n",
    "# Function to slice images into smaller patches, skipping urbanized areas\n",
    "def slice_images(image_pairs, output_dir, patch_size=64):\n",
    "    normal_dir = os.path.join(output_dir, \"normal\")\n",
    "    ndvi_dir = os.path.join(output_dir, \"ndvi\")\n",
    "    ensure_dir(normal_dir)\n",
    "    ensure_dir(ndvi_dir)\n",
    "\n",
    "    patch_id = 0\n",
    "\n",
    "    for true_color_path, ndvi_path in image_pairs:\n",
    "        img = cv2.imread(true_color_path)\n",
    "        ndvi = cv2.imread(ndvi_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if img is None or ndvi is None:\n",
    "            print(f\"Error loading images: {true_color_path}, {ndvi_path}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        urban_mask = get_urban_mask(true_color_path)  # Get urban mask from Segformer\n",
    "        urban_mask_resized = cv2.resize(urban_mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        ndvi_colormap = apply_reversed_summer_colormap(ndvi)\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        ph, pw = patch_size, patch_size\n",
    "\n",
    "        for i in range(0, h - ph + 1, ph):\n",
    "            for j in range(0, w - pw + 1, pw):\n",
    "                if urban_mask_resized[i:i + ph, j:j + pw].sum() > 0:  # Skip urban patches\n",
    "                    continue\n",
    "\n",
    "                true_color_patch = img[i:i + ph, j:j + pw]\n",
    "                ndvi_patch = ndvi_colormap[i:i + ph, j:j + pw]\n",
    "\n",
    "                true_color_patch_path = os.path.join(normal_dir, f\"patch_{patch_id}.png\")\n",
    "                ndvi_patch_path = os.path.join(ndvi_dir, f\"patch_{patch_id}.png\")\n",
    "\n",
    "                cv2.imwrite(true_color_patch_path, true_color_patch)\n",
    "                cv2.imwrite(ndvi_patch_path, ndvi_patch)\n",
    "\n",
    "                patch_id += 1\n",
    "\n",
    "    print(f\"Saved {patch_id} patches to {output_dir}\")\n",
    "\n",
    "# List of image pairs (true color, NDVI)\n",
    "image_pairs = [\n",
    "    (\"../color/1_True_color.jpg\", \"../ndvi/1_NDVI.jpg\"),\n",
    "    (\"../color/2_True_color.jpg\", \"../ndvi/2_NDVI.jpg\"),\n",
    "    (\"../color/3_True_color.jpg\", \"../ndvi/3_NDVI.jpg\"),\n",
    "    (\"../color/4_True_color.jpg\", \"../ndvi/4_NDVI.jpg\"),\n",
    "]\n",
    "\n",
    "# Output directory\n",
    "output_directory = \"combined_patches/\"\n",
    "\n",
    "# Slice images while skipping urban areas\n",
    "slice_images(image_pairs, output_directory, patch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_unet(input_shape=(64, 64, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    # Decoder\n",
    "    u1 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c3)\n",
    "    u1 = layers.concatenate([u1, c2])\n",
    "    c4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u1)\n",
    "    c4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "    u2 = layers.concatenate([u2, c1])\n",
    "    c5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c5)  # Single NDVI output channel\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c415eafb355c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joshu\\AppData\\Local\\r-miniconda\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 616ms/step - loss: 0.0452 - mae: 0.1790 - val_loss: 0.0478 - val_mae: 0.1860\n",
      "Epoch 2/6\n",
      "\u001b[1m 6/19\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 418ms/step - loss: 0.0383 - mae: 0.1683"
     ]
    }
   ],
   "source": [
    "class PatchGenerator(Sequence):\n",
    "    def __init__(self, normal_paths, ndvi_paths, batch_size):\n",
    "        self.normal_paths = sorted(normal_paths)\n",
    "        self.ndvi_paths = sorted(ndvi_paths)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.normal_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_normal = self.normal_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_ndvi = self.ndvi_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        for normal_path, ndvi_path in zip(batch_normal, batch_ndvi):\n",
    "            normal_img = cv2.imread(normal_path) / 255.0  # Normalize to [0, 1]\n",
    "            ndvi_img = cv2.imread(ndvi_path)[:, :, 1] / 255.0  # Extract green channel, normalize\n",
    "\n",
    "            X.append(normal_img)\n",
    "            Y.append(np.expand_dims(ndvi_img, axis=-1))  # Add channel dimension\n",
    "\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "# Paths to the patch directories\n",
    "normal_dir = \"combined_patches/normal\"\n",
    "ndvi_dir = \"combined_patches/ndvi\"\n",
    "\n",
    "# Get all image paths\n",
    "normal_paths = [os.path.join(normal_dir, f) for f in os.listdir(normal_dir)]\n",
    "ndvi_paths = [os.path.join(ndvi_dir, f) for f in os.listdir(ndvi_dir)]\n",
    "\n",
    "# Split the data into training and testing (80% training, 20% testing)\n",
    "train_normal_paths, test_normal_paths, train_ndvi_paths, test_ndvi_paths = train_test_split(\n",
    "    normal_paths, ndvi_paths, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Create data generators for training and testing\n",
    "batch_size = 16\n",
    "train_gen = PatchGenerator(train_normal_paths, train_ndvi_paths, batch_size)\n",
    "test_gen = PatchGenerator(test_normal_paths, test_ndvi_paths, batch_size)\n",
    "\n",
    "# Build the model\n",
    "model = build_unet(input_shape=(64, 64, 3))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4),\n",
    "              loss='mean_squared_error',  # For regression\n",
    "              metrics=['mae'])  # Optional: Monitor mean absolute error\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_gen, epochs=6, validation_data=test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d3cb5a8988e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_large_image(model, image_path, patch_size=64, stride=32, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Predict NDVI values for a large image using sliding windows,\n",
    "    apply a reversed COLORMAP_SUMMER for visualization,\n",
    "    and blend the prediction with the original image.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path).astype(np.float32) / 255.0  # Normalize input image to float32\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    # Get the urban segmentation mask\n",
    "    urban_mask = get_urban_mask(image_path)\n",
    "    urban_mask_resized = cv2.resize(urban_mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Pad image to make it divisible by patch_size\n",
    "    pad_h = (patch_size - h % patch_size) % patch_size\n",
    "    pad_w = (patch_size - w % patch_size) % patch_size\n",
    "    img_padded = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "\n",
    "    output = np.zeros((h + pad_h, w + pad_w, 1), dtype=np.float32)  # Ensure output is float32\n",
    "\n",
    "    # Sliding window over the image\n",
    "    for i in range(0, img_padded.shape[0] - patch_size + 1, stride):\n",
    "        for j in range(0, img_padded.shape[1] - patch_size + 1, stride):\n",
    "            patch = img_padded[i:i + patch_size, j:j + patch_size]\n",
    "            pred_patch = model.predict(np.expand_dims(patch, axis=0))[0]\n",
    "            output[i:i + patch_size, j:j + patch_size] += pred_patch\n",
    "\n",
    "    # Crop back to original size\n",
    "    output = output[:h, :w]\n",
    "\n",
    "    # Expand urban_mask_resized to have a third dimension\n",
    "    urban_mask_resized_expanded = urban_mask_resized[..., np.newaxis]\n",
    "\n",
    "    # Step 1: Apply the reversed colormap to the NDVI prediction\n",
    "    pred_colormap = apply_reversed_summer_colormap(output).astype(np.float32) / 255.0  # Normalize colormap\n",
    "\n",
    "    # Step 2: Blend the original image with the NDVI prediction\n",
    "    blended_image = cv2.addWeighted(pred_colormap, alpha, img, 1 - alpha, 0)\n",
    "\n",
    "    # Step 3: Combine the urban mask to finalize the result\n",
    "    # Non-urban areas (mask=0) get the blended image, urban areas (mask=1) keep original image values\n",
    "    final_image = blended_image * (1 - urban_mask_resized_expanded) + img * urban_mask_resized_expanded\n",
    "\n",
    "    return final_image\n",
    "\n",
    "# Predict NDVI on a large image\n",
    "large_image_path = \"../test/img.png\"\n",
    "ndvi_prediction = predict_large_image(model, large_image_path, alpha=0.4)\n",
    "\n",
    "# Save the final image if needed\n",
    "output_path = \"../test/img_NDVI_PRED_OVERLAY.png\"\n",
    "final_image = (ndvi_prediction * 255).astype(np.uint8)  # Convert back to 8-bit image for saving\n",
    "cv2.imwrite(output_path, final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c88b8241184d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
